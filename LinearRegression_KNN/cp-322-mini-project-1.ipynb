{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6704337,"sourceType":"datasetVersion","datasetId":3851908},{"sourceId":6766862,"sourceType":"datasetVersion","datasetId":3894428},{"sourceId":6766876,"sourceType":"datasetVersion","datasetId":3894434},{"sourceId":6766887,"sourceType":"datasetVersion","datasetId":3894441},{"sourceId":6767496,"sourceType":"datasetVersion","datasetId":3894668},{"sourceId":6767540,"sourceType":"datasetVersion","datasetId":3894683}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CP-322 Mini Project 1","metadata":{}},{"cell_type":"markdown","source":"# Imports\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-10-22T21:43:21.522707Z","iopub.execute_input":"2023-10-22T21:43:21.523079Z","iopub.status.idle":"2023-10-22T21:43:21.529224Z","shell.execute_reply.started":"2023-10-22T21:43:21.523049Z","shell.execute_reply":"2023-10-22T21:43:21.527622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 1","metadata":{}},{"cell_type":"code","source":"# Importing the data\n# Adult data\ndataAdult = pd.read_csv('/kaggle/input/adult-data-csv-2/adult.csv')\n\n# Ionosphere\n# ionosphere = pd.read_csv('/kaggle/input/ionosphere-data-csv/ionosphere_data_kaggle.csv')\nionosphere = pd.read_csv('/kaggle/input/ionosphere-data-csv-2/ionosphere_data.csv')\n\n# Iris data\niris = pd.read_csv('/kaggle/input/iris-data/Iris.data')\n# iris = pd.read_csv('/kaggle/input/iris/Iris.csv') # Dont change this just uncomment the one above\n\n# US cars data\n# us_cars = pd.read_csv('/kaggle/input/us-cars/USA_cars_datasets.csv')\nus_cars = pd.read_csv('/kaggle/input/usa-cers-dataset/USA_cars_datasets.csv') # Dont change this just uncomment the one above\n\ndatasets = {\n    'dataAdult': dataAdult.head(),\n    'ionosphere': ionosphere.head(),\n    'iris': iris.head(),\n    'us_cars': us_cars.head()\n}\n\ndatasets","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:16:32.556804Z","iopub.execute_input":"2023-10-22T22:16:32.557171Z","iopub.status.idle":"2023-10-22T22:16:32.567242Z","shell.execute_reply.started":"2023-10-22T22:16:32.557145Z","shell.execute_reply":"2023-10-22T22:16:32.565934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adult Data**","metadata":{}},{"cell_type":"code","source":"dataAdult.isin(['?']).sum()\n\ndataAdult['workclass']=dataAdult['workclass'].replace('?',np.nan)    \ndataAdult['occupation']=dataAdult['occupation'].replace('?',np.nan)  \ndataAdult['native.country']=dataAdult['native.country'].replace('?',np.nan)  \n# Drop the rows which has NaN rows \ndataAdult.dropna(how='any',inplace=True)\ndataAdult.dropna()\nprint(dataAdult.columns)\n\ndataAdult = dataAdult.drop(['education.num','age', 'hours.per.week', 'fnlwgt', 'capital.gain','capital.loss', 'native.country'], axis=1)\n\n#income\ndataAdult['income'] = dataAdult['income'].map({'<=50K': 0, '>50K': 1}).astype(int, errors='ignore')\n\n#gender\ndataAdult['sex'] = dataAdult['sex'].map({'Male': 0, 'Female': 1}).astype(int, errors='ignore')\n\n#race\ndataAdult['race'] = dataAdult['race'].map({'Black': 0, 'Asian-Pac-Islander': 1, 'Other': 2, 'White': 3, 'Amer-Indian-Eskimo': 4}).astype(int, errors='ignore')\n\n#marital\ndataAdult['marital.status'] = dataAdult['marital.status'].map({'Married-spouse-absent': 0, 'Widowed': 1, 'Married-civ-spouse': 2, 'Separated': 3, 'Divorced': 4,'Never-married': 5, 'Married-AF-spouse': 6}).astype(int, errors='ignore')\n\n#workclass\ndataAdult['workclass'] = dataAdult['workclass'].map({'Self-emp-inc': 0, 'State-gov': 1,'Federal-gov': 2, 'Without-pay': 3, 'Local-gov': 4,'Private': 5, 'Self-emp-not-inc': 6}).astype(int, errors='ignore')\n\n#education\ndataAdult['education'] = dataAdult['education'].map({'Some-college': 0, 'Preschool': 1, '5th-6th': 2, 'HS-grad': 3, 'Masters': 4, '12th': 5, '7th-8th': 6, 'Prof-school': 7,'1st-4th': 8, 'Assoc-acdm': 9, 'Doctorate': 10, '11th': 11,'Bachelors': 12, '10th': 13,'Assoc-voc': 14,'9th': 15}).astype(int, errors='ignore')\n\n#occupation\ndataAdult['occupation'] = dataAdult['occupation'].map({ 'Farming-fishing': 1, 'Tech-support': 2, 'Adm-clerical': 3, 'Handlers-cleaners': 4, \n 'Prof-specialty': 5,'Machine-op-inspct': 6, 'Exec-managerial': 7,'Priv-house-serv': 8,'Craft-repair': 9,'Sales': 10, 'Transport-moving': 11, 'Armed-Forces': 12, 'Other-service': 13,'Protective-     serv':14}).astype(int, errors='ignore')\n\n#relationship\ndataAdult['relationship'] = dataAdult['relationship'].map({'Not-in-family': 0, 'Wife': 1, 'Other-relative': 2, 'Unmarried': 3,'Husband': 4,'Own-child': 5}).astype(int, errors='ignore')\n\ndataAdult.dropna()\ndataAdult.columns\ndataAdult.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:38.158630Z","iopub.execute_input":"2023-10-22T19:57:38.159020Z","iopub.status.idle":"2023-10-22T19:57:38.382938Z","shell.execute_reply.started":"2023-10-22T19:57:38.158989Z","shell.execute_reply":"2023-10-22T19:57:38.381924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ionosphere**","metadata":{}},{"cell_type":"code","source":"# Cleaned ionosphere data\n\n#rename last column from g and b to 1 and 0\nionosphere['label'].replace('b', 0, inplace = True)\nionosphere['label'].replace('g', 1, inplace = True)\nionosphere\n#print(ionosphere.columns)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:41.746580Z","iopub.execute_input":"2023-10-22T19:57:41.746980Z","iopub.status.idle":"2023-10-22T19:57:41.791329Z","shell.execute_reply.started":"2023-10-22T19:57:41.746950Z","shell.execute_reply":"2023-10-22T19:57:41.790139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**US Cars Data**","metadata":{}},{"cell_type":"code","source":"# Cleaned US cars data\n#dropping an extra column\ncars_df = pd.DataFrame(us_cars)\ncars_df = cars_df.drop('Unnamed: 0', axis = 1)\n\n# Makes mileage float into an int\nfor i in cars_df['mileage']:\n    a = str(i)\n    if a[-1] != str(0):\n        print(a)       \ncars_df['mileage'] = cars_df['mileage'].astype(int)\n\ncars_df","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:45.078445Z","iopub.execute_input":"2023-10-22T19:57:45.078823Z","iopub.status.idle":"2023-10-22T19:57:45.110114Z","shell.execute_reply.started":"2023-10-22T19:57:45.078792Z","shell.execute_reply":"2023-10-22T19:57:45.108831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Iris Data**","metadata":{}},{"cell_type":"code","source":"iris","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:01:40.957349Z","iopub.execute_input":"2023-10-22T22:01:40.957703Z","iopub.status.idle":"2023-10-22T22:01:40.975404Z","shell.execute_reply.started":"2023-10-22T22:01:40.957673Z","shell.execute_reply":"2023-10-22T22:01:40.974461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iris.drop_duplicates(inplace=True)\n#replace species name with ints\niris['Species'].replace('Iris-setosa', 1, inplace = True)\niris['Species'].replace('Iris-versicolor', 2, inplace = True)\niris['Species'].replace('Iris-virginica', 3, inplace = True)\n# iris = iris[iris['Species'] != 'Iris-virginica']\n\niris.drop('Id', axis=1, inplace=True)\n\niris","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:16:57.793542Z","iopub.execute_input":"2023-10-22T22:16:57.794594Z","iopub.status.idle":"2023-10-22T22:16:57.813815Z","shell.execute_reply.started":"2023-10-22T22:16:57.794564Z","shell.execute_reply":"2023-10-22T22:16:57.812901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistics on the data sets","metadata":{}},{"cell_type":"code","source":"# Adult data\ndataAdult.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:50.451504Z","iopub.execute_input":"2023-10-22T19:57:50.451887Z","iopub.status.idle":"2023-10-22T19:57:50.497522Z","shell.execute_reply.started":"2023-10-22T19:57:50.451856Z","shell.execute_reply":"2023-10-22T19:57:50.496388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ionosphere\nionosphere.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:51.940136Z","iopub.execute_input":"2023-10-22T19:57:51.940883Z","iopub.status.idle":"2023-10-22T19:57:52.057204Z","shell.execute_reply.started":"2023-10-22T19:57:51.940833Z","shell.execute_reply":"2023-10-22T19:57:52.056003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iris \niris.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:53.179707Z","iopub.execute_input":"2023-10-22T19:57:53.180702Z","iopub.status.idle":"2023-10-22T19:57:53.212701Z","shell.execute_reply.started":"2023-10-22T19:57:53.180667Z","shell.execute_reply":"2023-10-22T19:57:53.211683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# US cars\nus_cars.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:54.851708Z","iopub.execute_input":"2023-10-22T19:57:54.852129Z","iopub.status.idle":"2023-10-22T19:57:54.881265Z","shell.execute_reply.started":"2023-10-22T19:57:54.852099Z","shell.execute_reply":"2023-10-22T19:57:54.880050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"code","source":"#logistic regression\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4):\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.tol = tol\n        self.theta = None\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def cost_function(self, y, y_prediction):\n        epsilon = 1e-15\n        return -np.mean(y * np.log(y_prediction + epsilon) + (1 - y) * np.log(1 - y_prediction + epsilon))\n\n    def gradient_descent(self, x, y):\n        n = len(y)\n        cost_history = []\n\n        for i in range(self.max_iter):\n            y_prediction = self.hypothesis(x)\n            gradient = np.dot(x.T, (y_prediction - y)) / n\n            self.theta -= self.learning_rate * gradient\n            cost = self.cost_function(y, y_prediction)\n            cost_history.append(cost)\n\n            # Check convergence\n            if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < self.tol:\n                break\n\n        return cost_history\n\n    def fit(self, x, y):\n        x_bias = np.hstack((np.ones((x.shape[0], 1)), x))\n        self.theta = np.zeros(x_bias.shape[1])\n        cost_history = self.gradient_descent(x_bias, y)\n        return cost_history\n\n    def predict(self, x):\n        x_bias = np.hstack((np.ones((x.shape[0], 1)), x))\n        probabilities = self.hypothesis(x_bias)\n        return [1 if p >= 0.5 else 0 for p in probabilities]\n\n    def hypothesis(self, x):\n        return self.sigmoid(np.dot(x, self.theta))\n\n    @staticmethod\n    def evaluate_acc(y_true, y_pred):\n        return np.sum(y_true == y_pred) / len(y_true)\n\n    #k-fold cross-validation for Logistic regression\n    @staticmethod\n    def cross_validate_logistic(X, y, n_splits):\n        # Shuffle the data\n        shuffled_data = pd.concat([X, y], axis=1).sample(frac=1).reset_index(drop=True)\n\n        # Split the data into k folds\n        fold_size = len(shuffled_data) // n_splits\n        accuracies = []\n\n        for i in range(n_splits):\n            # Split the data into training and validation sets for the current fold\n            start_idx = i * fold_size\n            end_idx = (i + 1) * fold_size if i != n_splits - 1 else None  # To handle the last fold\n\n            val_data = shuffled_data.iloc[start_idx:end_idx]\n            train_data = shuffled_data.drop(val_data.index)\n\n            # Separate the features and labels\n            X_train = train_data.drop(columns=y.name)\n            y_train = train_data[y.name]\n            X_val = val_data.drop(columns=y.name)\n            y_val = val_data[y.name]\n\n            # Train the model and validate\n            model = LogisticRegression()\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_val)\n            accuracy = np.mean(y_val == y_pred)\n            accuracies.append(accuracy)\n\n        return np.mean(accuracies)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:56.558712Z","iopub.execute_input":"2023-10-22T19:57:56.559188Z","iopub.status.idle":"2023-10-22T19:57:56.581956Z","shell.execute_reply.started":"2023-10-22T19:57:56.559155Z","shell.execute_reply":"2023-10-22T19:57:56.580643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Nearest neighbor KNN\nfrom collections import Counter\n\nclass KNN:\n    # initializing KNN\n    def __init__(self, k):\n        self.k = k\n    \n    # Fit function to store training data\n    def fit(self, X_train, y):\n        self.X_train = X_train\n        # Check if y is a pandas DataFrame or Series, if so, reset the index\n        if isinstance(y, (pd.DataFrame, pd.Series)):\n            self.y_train = y.reset_index(drop=True).values\n        else:\n            self.y_train = y\n\n    \n    # Computing the distance between two points using the Euclidean distance function\n    @staticmethod\n    def distance(x1, x2):\n        return np.linalg.norm(x1 - x2, axis=1)\n    \n    \"\"\"\n    Predict function runs a loop for every test data point, each time calculating distance between the test instance and every training instance. \n    It stores distance and index of the training data together in a 2D list. \n    Then sorts that list based on distance and then updates the list keeping only the K shortest distances(along with their indices) in the list.\n    Then pulls out labels corresponding to those K nearest data points and checks which label has the majority using Counter. \n    That majority label becomes the label of the test data point.\n    \"\"\"\n    def predict(self, X_test):\n        X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n        predictions = []\n        for x in X_test:\n            distances = self.distance(self.X_train, x)\n            k_indices = distances.argsort()[:self.k]  # Get the indices of the k smallest distances\n            k_nearest_labels = self.y_train[k_indices]\n            most_common = np.bincount(k_nearest_labels).argmax()\n            predictions.append(most_common)\n        return predictions\n        \n# Function to run k-fold cross-validation for KNN\ndef cross_validation_knn(X,y, k_list, n_splits):\n    # Check if X and y are pandas DataFrame or Series, if so, convert them to numpy arrays\n    if isinstance(X, (pd.DataFrame, pd.Series)):\n        X = X.values\n    if isinstance(y, (pd.DataFrame, pd.Series)):\n        y = y.values\n    fold_size = len(X) // n_splits  # size of each fold\n    indices = np.arange(len(X))  # create an array\n    np.random.shuffle(indices)  # shuffle the indices\n\n    k_accuracies = {}  # Dictionary to store accuracies for each k\n\n    for k in k_list:\n        accuracies = []\n\n        for i in range(n_splits):\n            # Splitting the data into training and validation based on current fold\n            test_indices = indices[i * fold_size:(i + 1) * fold_size]\n            train_indices = np.setdiff1d(indices, test_indices)\n\n            X_train, X_val = X[train_indices], X[test_indices]\n            y_train, y_val = y[train_indices], y[test_indices]\n\n            # Initializing KNN and training\n            knn = KNN(k=k)\n            knn.fit(X_train, y_train)\n            y_pred = knn.predict(X_val)\n\n            # Evaluating accuracy\n            accuracy = np.sum(y_val == y_pred) / len(y_val)\n            accuracies.append(accuracy)\n\n        # Storing mean accuracy for the current k value\n        k_accuracies[k] = np.mean(accuracies)\n\n    return k_accuracies\n# Accuracy function\ndef evaluate_acc_knn(y_true, y_pred):\n    # check if both have the same length\n    if len(y_true) != len(y_pred):\n        print(\"Input arrays must have the same length\")\n    else:\n        return np.sum(y_true == y_pred)/len(y_true)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:57:59.373046Z","iopub.execute_input":"2023-10-22T19:57:59.373493Z","iopub.status.idle":"2023-10-22T19:57:59.394507Z","shell.execute_reply.started":"2023-10-22T19:57:59.373459Z","shell.execute_reply":"2023-10-22T19:57:59.393255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 3","metadata":{}},{"cell_type":"markdown","source":"# Testing Adult Data","metadata":{}},{"cell_type":"code","source":"# Selecting relevant columns for x and y\ndf_x = dataAdult[['relationship','education','race','occupation','sex','marital.status','workclass']]\ndf_y = dataAdult['income']\n\n# Splitting the dataset into training and testing sets (80-20 split)\nnp.random.seed(42)\nshuffled_indices = np.random.permutation(len(df_x))\n\n# Determine the split index based on the desired ratio (80-20 split)\ntrain_indices = shuffled_indices[:int(0.8 * len(df_x))]\ntest_indices = shuffled_indices[int(0.8 * len(df_x)):]\n\n# Split the data based on the shuffled indices\nx_train = df_x.iloc[train_indices]\nx_test = df_x.iloc[test_indices]\ny_train = df_y.iloc[train_indices]\ny_test = df_y.iloc[test_indices]\n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:58:02.937585Z","iopub.execute_input":"2023-10-22T19:58:02.938024Z","iopub.status.idle":"2023-10-22T19:58:02.959569Z","shell.execute_reply.started":"2023-10-22T19:58:02.937990Z","shell.execute_reply":"2023-10-22T19:58:02.958339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.fillna(x_train.mean(), inplace=True)\nx_test.fillna(x_train.mean(), inplace=True)\n\n# Check for NaN or inf values in the training and testing datasets\nnan_in_train = x_train.isnull().sum().sum()\nnan_in_test = x_test.isnull().sum().sum()\n\nprint(\"NaN in training:\", nan_in_train)\nprint(\"NaN in testing:\", nan_in_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:58:06.612356Z","iopub.execute_input":"2023-10-22T19:58:06.612721Z","iopub.status.idle":"2023-10-22T19:58:06.637268Z","shell.execute_reply.started":"2023-10-22T19:58:06.612693Z","shell.execute_reply":"2023-10-22T19:58:06.636074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"logisticModel = LogisticRegression()\nlogisticModel.fit(x_train, y_train)\n\npredictions = logisticModel.predict(x_test)\n#print(\"predictions: \",predictions)\n\n# checking the accuracy of the model\naccuracy = LogisticRegression.evaluate_acc(y_test, predictions)\nprint(\"Accuracy: \", accuracy)\n\n# running the cross-validation function\navg_accuracy = LogisticRegression.cross_validate_logistic(x_train, y_train, n_splits=5)\nprint(f\"The average accuracy from k-fold cross-validation is: {avg_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:58:14.805195Z","iopub.execute_input":"2023-10-22T19:58:14.805564Z","iopub.status.idle":"2023-10-22T19:58:20.881334Z","shell.execute_reply.started":"2023-10-22T19:58:14.805536Z","shell.execute_reply":"2023-10-22T19:58:20.879915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The models performance on the validation sets during k-fold cross-validation is quite close to its performance on the test set. This is a good sign as it indicates consistency in the model's performance across different data splits.","metadata":{}},{"cell_type":"code","source":"# Testing different learning rates and plotting the cost history\nlearning_rates = [0.00001, 0.0001, 0.001, 0.01]\ncost_histories = {}\n\nfor lr in learning_rates:\n    model = LogisticRegression(learning_rate=lr)\n    cost_histories[lr] = model.fit(x_train, y_train)\n\n# Plotting the cost histories for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, history in cost_histories.items():\n    plt.plot(history, label=f\"Learning Rate: {lr}\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost History for Different Learning Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:58:27.703192Z","iopub.execute_input":"2023-10-22T19:58:27.703588Z","iopub.status.idle":"2023-10-22T19:58:32.175958Z","shell.execute_reply.started":"2023-10-22T19:58:27.703558Z","shell.execute_reply":"2023-10-22T19:58:32.174992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"KNearNeighbor = KNN(k=5)  # Using k=5\nKNearNeighbor.fit(x_train, y_train)\n\n# Predict the labels for the test data\nknn_predictions = KNearNeighbor.predict(x_test)\n#print(\"KNN Predictions: \", knn_predictions)\n\n# Evaluate the accuracy of the KNN model using the test data\nknn_accuracy = evaluate_acc_knn(y_test, knn_predictions)\nprint(\"KNN Accuracy: \", knn_accuracy)\n\n# Run k-fold cross-validation for the KNN model\nk_list = list(range(1,6))  # You can modify this list to test different values of k, in this case its 1-5\nk_accuracies = cross_validation_knn(x_train, y_train, k_list, n_splits=5)\n\n# Print the accuracies for different values of k\nfor k, acc in k_accuracies.items():\n    print(f\"CV Accuracy for k={k}: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T19:58:40.406226Z","iopub.execute_input":"2023-10-22T19:58:40.406607Z","iopub.status.idle":"2023-10-22T20:03:42.012262Z","shell.execute_reply.started":"2023-10-22T19:58:40.406578Z","shell.execute_reply":"2023-10-22T20:03:42.010902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy of approximately 80.06% for the KNN algorithm","metadata":{}},{"cell_type":"code","source":"k_values = list(range(1, 21))  # Testing k from 1 to 20\naccuracies = []\n\n# For each k value, train the KNN model and evaluate its accuracy\nfor k in k_values:\n    knnModelTemp = KNN(k=k)\n    knnModelTemp.fit(x_train, y_train)\n    knn_predictions_temp = knnModelTemp.predict(x_test)\n    accuracy = evaluate_acc_knn(y_test, knn_predictions_temp)\n    accuracies.append(accuracy)\n\n# Plot k values against accuracies\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracies, marker='o', linestyle='-')\nplt.xlabel('k Value')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. k Value for KNN')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:04:48.283678Z","iopub.execute_input":"2023-10-22T20:04:48.284837Z","iopub.status.idle":"2023-10-22T20:11:37.593752Z","shell.execute_reply.started":"2023-10-22T20:04:48.284787Z","shell.execute_reply":"2023-10-22T20:11:37.592625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy seems to be increasing steadily until the 9th k-value and then from there it dips approximately 1-2% until the 15th k-value and then increases from there on. While increasing k initially leads to a more stable model by reducing sensitivity to noise, there's a tipping point beyond which the model starts losing accuracy. It appears this tipping point is around the 9th k-value.","metadata":{}},{"cell_type":"markdown","source":"# Testing Ionosphere Data","metadata":{}},{"cell_type":"code","source":"#Train-test split\nX_iono = ionosphere[['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6',\n       'feature7', 'feature8', 'feature9', 'feature10', 'feature11',\n       'feature12', 'feature13', 'feature14', 'feature15', 'feature16',\n       'feature17', 'feature18', 'feature19', 'feature20', 'feature21',\n       'feature22', 'feature23', 'feature24', 'feature25', 'feature26',\n       'feature27', 'feature28', 'feature29', 'feature30', 'feature31',\n       'feature32', 'feature33', 'feature34']]\ny_iono = ionosphere['label']\n\n#80-20 ratio\ntrain_size = int(0.8 * len(ionosphere))\nx_train_iono, x_test_iono = X_iono[:train_size], X_iono[train_size:]\ny_train_iono, y_test_iono = y_iono[:train_size], y_iono[train_size:]","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:11:57.690984Z","iopub.execute_input":"2023-10-22T20:11:57.691492Z","iopub.status.idle":"2023-10-22T20:11:57.702011Z","shell.execute_reply.started":"2023-10-22T20:11:57.691458Z","shell.execute_reply":"2023-10-22T20:11:57.700624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"logisticModel = LogisticRegression()\nlogisticModel.fit(x_train_iono, y_train_iono)\n\ncost_history = logisticModel.fit(x_train_iono, y_train_iono)\n#print(\"cost history: \",cost_history)\n\npredictions = logisticModel.predict(x_test_iono)\n#print(\"predictions: \",predictions)\n\n# checking the accuracy of the model\naccuracy = logisticModel.evaluate_acc(y_test_iono, predictions)\nprint(\"Accuracy: \", accuracy)\n\n# running the cross-validation function\navg_accuracy = LogisticRegression.cross_validate_logistic(X_iono, y_iono, 3)\nprint(f\"The average accuracy from k-fold cross-validation is: {avg_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:12:00.541031Z","iopub.execute_input":"2023-10-22T20:12:00.541437Z","iopub.status.idle":"2023-10-22T20:12:07.140742Z","shell.execute_reply.started":"2023-10-22T20:12:00.541404Z","shell.execute_reply":"2023-10-22T20:12:07.139612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing different learning rates and plotting the cost history\nlearning_rates = [0.01, 0.05, 0.1, 0.5, 1]\ncost_histories = {}\n\nfor lr in learning_rates:\n    model = LogisticRegression(learning_rate=lr)\n    cost_histories[lr] = model.fit(x_train_iono, y_train_iono)\n\n# Plotting the cost histories for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, history in cost_histories.items():\n    plt.plot(history, label=f\"Learning Rate: {lr}\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost History for Different Learning Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:48:16.348034Z","iopub.execute_input":"2023-10-22T20:48:16.348450Z","iopub.status.idle":"2023-10-22T20:48:22.866892Z","shell.execute_reply.started":"2023-10-22T20:48:16.348417Z","shell.execute_reply":"2023-10-22T20:48:22.865791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"KNearNeighbor = KNN(k=5)  # Using k=5\nKNearNeighbor.fit(x_train_iono, y_train_iono)\n\n# Predict the labels for the test data\nknn_predictions = KNearNeighbor.predict(x_test_iono)\n#print(\"KNN Predictions: \", knn_predictions)\n\n# Evaluate the accuracy of the KNN model using the test data\nknn_accuracy = evaluate_acc_knn(y_test_iono, knn_predictions)\nprint(\"KNN Accuracy: \", knn_accuracy)\n\n# Run k-fold cross-validation for the KNN model\nk_list = list(range(1,6))  # You can modify this list to test different values of k, in this case its 1-5\nk_accuracies = cross_validation_knn(x_train_iono, y_train_iono, k_list, n_splits=5)\n\n# Print the accuracies for different values of k\nfor k, acc in k_accuracies.items():\n    print(f\"CV Accuracy for k={k}: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:12:20.278896Z","iopub.execute_input":"2023-10-22T20:12:20.279289Z","iopub.status.idle":"2023-10-22T20:12:20.417759Z","shell.execute_reply.started":"2023-10-22T20:12:20.279257Z","shell.execute_reply":"2023-10-22T20:12:20.416637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_values = list(range(1, 21))  # Testing k from 1 to 20\naccuracies = []\n\n# For each k value, train the KNN model and evaluate its accuracy\nfor k in k_values:\n    knnModelTemp = KNN(k=k)\n    knnModelTemp.fit(x_train_iono, y_train_iono)\n    knn_predictions_temp = knnModelTemp.predict(x_test_iono)\n    accuracy = evaluate_acc_knn(y_test_iono, knn_predictions_temp)\n    accuracies.append(accuracy)\n\n# Plot k values against accuracies\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracies, marker='o', linestyle='-')\nplt.xlabel('k Value')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. k Value for KNN')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:18:04.219808Z","iopub.execute_input":"2023-10-22T20:18:04.220579Z","iopub.status.idle":"2023-10-22T20:18:05.440697Z","shell.execute_reply.started":"2023-10-22T20:18:04.220537Z","shell.execute_reply":"2023-10-22T20:18:05.439528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Iris Data","metadata":{}},{"cell_type":"code","source":"X_iris = iris.iloc[:,0:4]\n\ntrain_size = int(0.8 * len(iris))\nx_train_iris, x_test_iris = X_iris[:train_size], X_iris[train_size:]\ny_train_iris, y_test_iris = Y_iris[:train_size], Y_iris[train_size:]\n\niris","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:19:55.065384Z","iopub.execute_input":"2023-10-22T22:19:55.065737Z","iopub.status.idle":"2023-10-22T22:19:55.086261Z","shell.execute_reply.started":"2023-10-22T22:19:55.065709Z","shell.execute_reply":"2023-10-22T22:19:55.084702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"logisticModel = LogisticRegression(learning_rate = 0.001)\nlogisticModel.fit(x_train_iris, y_train_iris)\n\ncost_history = logisticModel.fit(x_train_iris, y_train_iris)\n# print(\"cost history: \",cost_history)\n\npredictions = logisticModel.predict(x_test_iris)\nprint(\"predictions: \",predictions)\n\n# checking the accuracy of the model\naccuracy = logisticModel.evaluate_acc(y_test_iris, predictions)\nprint(\"Accuracy: \", accuracy)\n\n# running the cross-validation function\navg_accuracy = LogisticRegression.cross_validate_logistic(X_iris, Y_iris, 5)\nprint(f\"The average accuracy from k-fold cross-validation is: {avg_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:19:56.564815Z","iopub.execute_input":"2023-10-22T22:19:56.565182Z","iopub.status.idle":"2023-10-22T22:19:57.746685Z","shell.execute_reply.started":"2023-10-22T22:19:56.565155Z","shell.execute_reply":"2023-10-22T22:19:57.745797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing different learning rates and plotting the cost history\nlearning_rates = [0.000001, 0.00001, 0.0001, 0.001]\ncost_histories = {}\n\nfor lr in learning_rates:\n    model = LogisticRegression(learning_rate=lr)\n    cost_histories[lr] = model.fit(x_train_iris, y_train_iris)\n\n# Plotting the cost histories for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, history in cost_histories.items():\n    plt.plot(history, label=f\"Learning Rate: {lr}\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost History for Different Learning Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:17:18.109838Z","iopub.execute_input":"2023-10-22T22:17:18.110474Z","iopub.status.idle":"2023-10-22T22:17:20.604480Z","shell.execute_reply.started":"2023-10-22T22:17:18.110450Z","shell.execute_reply":"2023-10-22T22:17:20.603316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"KNearNeighbor = KNN(k=5)  # Using k=5\nKNearNeighbor.fit(x_train_iris, y_train_iris)\n\n# Predict the labels for the test data\nx_test_iris = x_test_iris.astype(float)\nknn_predictions = KNearNeighbor.predict(x_test_iris)\n#print(\"KNN Predictions: \", knn_predictions)\n\n# Evaluate the accuracy of the KNN model using the test data\nknn_accuracy = evaluate_acc_knn(y_test_iris, knn_predictions)\nprint(\"KNN Accuracy: \", knn_accuracy)\n\n# Run k-fold cross-validation for the KNN model\nk_list = list(range(1,6))  # You can modify this list to test different values of k, in this case its 1-5\nk_accuracies = cross_validation_knn(x_train_iris, y_train_iris, k_list, n_splits=5)\n\n# Print the accuracies for different values of k\nfor k, acc in k_accuracies.items():\n    print(f\"CV Accuracy for k={k}: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:20:08.127409Z","iopub.execute_input":"2023-10-22T22:20:08.127794Z","iopub.status.idle":"2023-10-22T22:20:08.165637Z","shell.execute_reply.started":"2023-10-22T22:20:08.127756Z","shell.execute_reply":"2023-10-22T22:20:08.164204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_values = list(range(1, 21))  # Testing k from 1 to 20\naccuracies = []\n\n# For each k value, train the KNN model and evaluate its accuracy\nfor k in k_values:\n    knnModelTemp = KNN(k=k)\n    knnModelTemp.fit(x_train_iris, y_train_iris)\n    knn_predictions_temp = knnModelTemp.predict(x_test_iris)\n    accuracy = evaluate_acc_knn(y_test_iris, knn_predictions_temp)\n    accuracies.append(accuracy)\n\n# Plot k values against accuracies\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracies, marker='o', linestyle='-')\nplt.xlabel('k Value')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. k Value for KNN for Iris dataset')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T22:20:10.318588Z","iopub.execute_input":"2023-10-22T22:20:10.320185Z","iopub.status.idle":"2023-10-22T22:20:11.192382Z","shell.execute_reply.started":"2023-10-22T22:20:10.320131Z","shell.execute_reply":"2023-10-22T22:20:11.191010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing US Cars Data","metadata":{}},{"cell_type":"code","source":"# Clean the data and drop some irrelavant columns\ncars_df = pd.DataFrame(us_cars)\ncars_df = cars_df.drop('Unnamed: 0', axis = 1)\ncars_df.drop(columns=['vin', 'lot', 'country'], inplace = True)\ncars_df.drop(columns=['brand', 'model', 'color', 'state', 'condition'], inplace=True)\n\n\n# Make sure the mileage is the right datatype\ncars_df['mileage'] = cars_df ['mileage'].astype(int)\ntitle_status = (cars_df.pop('title_status'))\ncars_df.insert(3, 'title_status', title_status)\n\ncars_df['title_status'] = cars_df['title_status'].replace({'clean vehicle': 1, 'salvage insurance': 0})\n\n#80-20 ratio\ny_cars = cars_df['title_status']\nX_cars = cars_df[['year', 'mileage', 'price']]\n\ntrain_size = int(0.8 * len(cars_df))\nx_train_cars, x_test_cars = X_cars[:train_size], X_cars[train_size:]\ny_train_cars, y_test_cars = y_cars[:train_size], y_cars[train_size:]","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:28:59.544169Z","iopub.execute_input":"2023-10-22T20:28:59.544555Z","iopub.status.idle":"2023-10-22T20:28:59.564351Z","shell.execute_reply.started":"2023-10-22T20:28:59.544525Z","shell.execute_reply":"2023-10-22T20:28:59.563261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"# # cars_df\nlogisticModel = LogisticRegression(learning_rate = 0.00001)\nlogisticModel.fit(x_train_cars, y_train_cars)\n\ncost_history = logisticModel.fit(x_train_cars, y_train_cars)\n#print(\"cost history: \",cost_history)\n\npredictions = logisticModel.predict(x_test_cars)\n#print(\"predictions: \",predictions)\n\n# # checking the accuracy of the model\naccuracy = logisticModel.evaluate_acc(y_test_cars, predictions)\nprint(\"Accuracy: \", str(accuracy *100) + \"%\")\n\navg_accuracy = LogisticRegression.cross_validate_logistic(X_cars, y_cars, 3)\nprint(f\"The average accuracy from k-fold cross-validation is: \" + str(avg_accuracy*100) + \"%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T21:13:42.722381Z","iopub.execute_input":"2023-10-22T21:13:42.722769Z","iopub.status.idle":"2023-10-22T21:13:42.774057Z","shell.execute_reply.started":"2023-10-22T21:13:42.722739Z","shell.execute_reply":"2023-10-22T21:13:42.773312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing different learning rates and plotting the cost history\nlearning_rates = [0.00001, 0.0001, 0.001, 0.01]\ncost_histories = {}\n\nfor lr in learning_rates:\n    model = LogisticRegression(learning_rate=lr)\n    cost_histories[lr] = model.fit(x_train_cars, y_train_cars)\n\n# Plotting the cost histories for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, history in cost_histories.items():\n    plt.plot(history, label=f\"Learning Rate: {lr}\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost History for Different Learning Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T21:10:50.294972Z","iopub.execute_input":"2023-10-22T21:10:50.295331Z","iopub.status.idle":"2023-10-22T21:10:50.631703Z","shell.execute_reply.started":"2023-10-22T21:10:50.295307Z","shell.execute_reply":"2023-10-22T21:10:50.630014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"KNearNeighbor = KNN(k=3)  # Using k=3 as default = KNN(k=3)  # Using k=3 as default\nKNearNeighbor.fit(x_train_cars, y_train_cars)\n\n# 2. Predict the labels for the test data\nknn_predictions = KNearNeighbor.predict(x_test_cars)\nprint(\"KNN Predictions: \", knn_predictions)\n\n# 3. Evaluate the accuracy of the KNN model using the test data\nknn_accuracy = evaluate_acc_knn(y_test_cars, knn_predictions)\nprint(\"KNN Accuracy: \" + str(knn_accuracy*100) + \"%\")\n\n# 4. Run k-fold cross-validation for the KNN model\nk_list = [1, 3, 5, 7, 9, 11]  # You can modify this list to test different values of k\nk_accuracies = cross_validation_knn(X_cars, y_cars, k_list, 3)\n\n# Print the accuracies for different values of k\nfor k, acc in k_accuracies.items():\n    print(f\"Accuracy for k={k}: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:29:09.968719Z","iopub.execute_input":"2023-10-22T20:29:09.969142Z","iopub.status.idle":"2023-10-22T20:29:13.633546Z","shell.execute_reply.started":"2023-10-22T20:29:09.969108Z","shell.execute_reply":"2023-10-22T20:29:13.632283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_values = list(range(1, 21))  # Testing k from 1 to 20\naccuracies = []\n\n# For each k value, train the KNN model and evaluate its accuracy\nfor k in k_values:\n    knnModelTemp = KNN(k=k)\n    knnModelTemp.fit(x_train_cars, y_train_cars)\n    knn_predictions_temp = knnModelTemp.predict(x_test_cars)\n    accuracy = evaluate_acc_knn(y_test_cars, knn_predictions_temp)\n    accuracies.append(accuracy)\n\n# Plot k values against accuracies\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracies, marker='o', linestyle='-')\nplt.xlabel('k Value')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. k Value for KNN')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T20:31:08.980455Z","iopub.execute_input":"2023-10-22T20:31:08.980998Z","iopub.status.idle":"2023-10-22T20:31:16.983314Z","shell.execute_reply.started":"2023-10-22T20:31:08.980952Z","shell.execute_reply":"2023-10-22T20:31:16.982001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Writeup\n## Abstract\nIn this project, we have used two classification algorithms, Logistic Regression and K-Nearest Neighbors(KNN) on four different datasets, Ionosphere, Adult dataset, Youtube Spam Comments dataset and Online Retail dataset. The main focus was to evaluate and compare the performance of these algorithms in terms of accuracy and training efficiensy. Cleaning and tidying of the datasets was also performed in order for us to have a better understanding of the features within each dataset. The optimal hyperparameters (a=0.01, k=3) were chosen for both algorithms and after the data processing and model implementation (After we have done our testing we can mention our results here) Either logistic regression had a faster convergence or KNN had a higher accuracy rate.\n\n## Introduction\n- Explain more in depth the Logistic regression function and KNN (maybe explain what the methods do in terms of coding), what they do, how they act on datasets\n- Maybe some background on why we chose the specific datasets\n- Mention some more important findings from the final results when the testing is done\n\n## Datasets\n- Explain how they were cleaned and how tidying was done, im guessing we have to mention the datasets that we chose cause the other ones were relatively clean\n\n## Results\n- A discussion of how the logistic regression performance (e.g., convergence speed) depends on the learning rate. (Note: a figure would be an ideal way to report these results).\n- A figure with explanations showing how you chose the best k for your k-nearest neighbor approach.\n- A comparison of the accuracy of k-nearest neighbor and logistic regression on datasets.\n- Results demonstrating that the feature subset and/or new features you used improved performance.\n\n\n## Discussion and Conclusion\n- Summarize the key takeaways from the project and possibly directions for future investigation\n\n## Statement of Contributions\n- Alexandros Ioannou:\n- Đức Nguyễn Minh:\n- Florian Novak:\n- Saumya Patel:","metadata":{}}]}